import json
import logging
import os
import random
import re
import secrets
from typing import Dict, List, Optional, Tuple

import pandas as pd
import torch
import torch.nn.functional as F
from torch import nn
from transformers import AutoModelForCausalLM, AutoTokenizer

from gpt_exploit_generation import GPTExploitGenerator
from target_validator import TargetValidator

# Optional security modules - disabled for basic functionality
# from kernel_memory_protection import KernelMemoryProtection
# from advanced_anti_analysis import AdvancedAntiAnalysis


class PolymorphicCodeGenerator:
    """Handles polymorphic code generation and mutation"""

    def __init__(self):
        self.mutation_techniques = {
            "variable_renaming": self._mutate_variables,
            "code_reordering": self._reorder_code_blocks,
            "instruction_substitution": self._substitute_instructions,
            "dead_code_insertion": self._insert_dead_code,
            "control_flow_obfuscation": self._obfuscate_control_flow,
        }

    def _mutate_variables(self, code: str) -> str:
        """Rename variables while preserving functionality"""
        var_pattern = r"\b(?:var|let|const)\s+([a-zA-Z_]\w*)"
        var_matches = re.finditer(var_pattern, code)
        var_mapping = {}

        for match in var_matches:
            old_name = match.group(1)
            if old_name not in var_mapping:
                new_name = f"v_{random.randint(1000, 9999)}"
                var_mapping[old_name] = new_name

        for old_name, new_name in var_mapping.items():
            code = re.sub(r"\b" + old_name + r"\b", new_name, code)
        return code

    def _reorder_code_blocks(self, code: str) -> str:
        """Reorder independent code blocks"""
        blocks = code.split("\n\n")
        if len(blocks) > 1:
            # Only reorder blocks that don't have dependencies
            independent_blocks = []
            dependent_blocks = []

            for block in blocks:
                if not any(dep in block for dep in ["import", "from", "def", "class"]):
                    independent_blocks.append(block)
                else:
                    dependent_blocks.append(block)

            random.shuffle(independent_blocks)
            blocks = dependent_blocks + independent_blocks

        return "\n\n".join(blocks)

    def _substitute_instructions(self, code: str) -> str:
        """Replace instructions with equivalent alternatives"""
        substitutions = {
            r"(\w+)\s*\+=\s*1": r"\1 = \1 + 1",
            r"(\w+)\s*-=\s*1": r"\1 = \1 - 1",
            r"while\s*\(true\)": r"for(;;)",
            r"array\.length": r'array["length"]',
        }

        for pattern, replacement in substitutions.items():
            code = re.sub(pattern, replacement, code)
        return code

    def _insert_dead_code(self, code: str) -> str:
        """Insert benign code that doesn't affect functionality"""
        dead_code_templates = [
            '\nif False: print("Unreachable")',
            "\nif 0: pass",
            "\ntry: pass\nexcept: pass",
            "\n# Polymorphic marker: {}".format(random.randint(1000, 9999)),
        ]

        lines = code.split("\n")
        for _ in range(random.randint(2, 5)):
            pos = random.randint(0, len(lines))
            lines.insert(pos, random.choice(dead_code_templates))

        return "\n".join(lines)

    def _obfuscate_control_flow(self, code: str) -> str:
        """Obfuscate control flow while preserving logic"""
        flow_patterns = {
            r"if\s+(.+):": lambda m: f"if bool({m.group(1)}) is True:",
            r"while\s+(.+):": lambda m: f"while bool({m.group(1)}) is not False:",
            r"for\s+(.+)\s+in\s+(.+):": lambda m: f"for {m.group(1)} in list({m.group(2)}):",
        }

        for pattern, replacement in flow_patterns.items():
            code = re.sub(pattern, replacement, code)
        return code

    def generate_polymorphic_variant(
        self, code: str, mutation_rate: float = 0.7
    ) -> str:
        """Generate a polymorphic variant of the given code"""
        techniques = list(self.mutation_techniques.values())
        num_mutations = int(len(techniques) * mutation_rate)
        selected_techniques = random.sample(techniques, num_mutations)

        mutated_code = code
        for technique in selected_techniques:
            try:
                mutated_code = technique(mutated_code)
            except Exception as e:
                logging.warning(f"Mutation technique failed: {str(e)}")
                continue

        return mutated_code


class ExploitGenerator:
    def __init__(
        self,
        model_name: str = "gpt-neo-125M",
        enable_gpt: bool = True,
        enable_polymorphic: bool = True,
        enable_autonomous_learning: bool = True,
    ):
        """Initialize the exploit generator with specified model and features"""
        self.enable_gpt = enable_gpt
        self.enable_polymorphic = enable_polymorphic
        self.enable_autonomous_learning = enable_autonomous_learning

        if enable_polymorphic:
            self.polymorphic_generator = PolymorphicCodeGenerator()
        if enable_gpt:
            self.gpt_generator = GPTExploitGenerator()  # Initialize GPT-based generator

        # Initialize stealth features
        self.stealth_features = {
            "anti_forensics": False,
            "sandbox_detection": False,
            "kernel_stealth": False,
            "memory_protection": False,
        }

        # Optional security components - disabled for basic functionality
        # self.memory_protection = KernelMemoryProtection()
        # self.anti_analysis = AdvancedAntiAnalysis()
        # Setup logging without basicConfig to avoid conflicts
        self.logger = logging.getLogger("ExploitGenerator")
        if not self.logger.handlers:
            handler = logging.FileHandler("exploit_generation.log")
            handler.setFormatter(
                logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
            )
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

        try:
            self.logger.info(f"Loading tokenizer from {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.logger.info("Tokenizer loaded successfully")

            self.logger.info(f"Loading model from {model_name}")
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            self.logger.info("Model loaded successfully")

            self.model.eval()  # Set to evaluation mode
            self.logger.info("Model set to evaluation mode")
        except Exception as e:
            self.logger.error(f"Error initializing model: {str(e)}")
            raise

        # Initialize other attributes
        self.learned_patterns = {}
        self.success_metrics = {
            "generated_count": 0,
            "success_rate": 0.0,
            "pattern_matches": 0,
        }

        # Initialize target validator
        self.target_validator = TargetValidator()
        self.learned_patterns = {}
        self.success_metrics = {
            "generated_count": 0,
            "success_rate": 0.0,
            "pattern_matches": 0,
        }

        # Initialize target validator
        self.target_validator = TargetValidator()

        # Setup logging
        logging.basicConfig(
            filename="exploit_generation.log",
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger("ExploitGenerator")

    def load_templates(self, template_file: str) -> None:
        """Load exploit templates from CSV file"""
        try:
            self.templates_df = pd.read_csv(template_file)
            print(f"Loaded {len(self.templates_df)} exploit templates")
        except Exception as e:
            print(f"Error loading templates: {e}")
            self.templates_df = pd.DataFrame()

    def _find_matching_template(self, vulnerability_info: Dict) -> Optional[str]:
        """Find best matching template for given vulnerability"""
        if self.templates_df.empty:
            return None

        best_match = None
        highest_score = 0

        for _, template in self.templates_df.iterrows():
            score = self._calculate_similarity(vulnerability_info, template)
            if score > highest_score:
                highest_score = score
                best_match = template["exploit_template"]

        return best_match if highest_score > 0.5 else None

    def _calculate_similarity(self, vuln_info: Dict, template: pd.Series) -> float:
        """Calculate similarity between vulnerability and template"""
        score = 0.0
        if (
            "type" in vuln_info
            and vuln_info["type"].lower() in template["description"].lower()
        ):
            score += 0.3
        if "description" in vuln_info and any(
            word in template["description"].lower()
            for word in vuln_info["description"].lower().split()
        ):
            score += 0.4
        if "cve_id" in vuln_info and vuln_info["cve_id"] == template["cve_id"]:
            score += 0.3
        return score

    def _validate_generated_code(self, code: str) -> bool:
        """Validate generated exploit code for safety, correctness, and stealth capabilities"""
        # Basic safety checks
        # Note: Advanced security checks disabled for basic functionality
        # Anti-analysis and memory protection features can be re-enabled when components are available
        dangerous_patterns = [
            r"rm\s+-rf",
            r"shutdown",
            r"format",
            r"mkfs",
            r"dd\s+if=",
            r"system\(\s*['\"]rm",
            r"system\(\s*['\"]format",
            r"system\(\s*['\"]mkfs",
            r"unlink\(\s*['\"]\/",
            r"exec\(\s*['\"]rm",
        ]

        # Required stealth features
        stealth_features = {
            "process_hiding": [
                r"ptrace",
                r"process\.kill",
                r"hidden_process",
                r"stealth_mode",
            ],
            "memory_manipulation": [
                r"mmap",
                r"virtualalloc",
                r"memcpy",
                r"memory_inject",
                r"memory_encrypt",
                r"memory_segment_protection",
                r"kernel_memory_isolation",
                r"page_guard_implementation",
            ],
            "anti_debugging": [
                r"isdebuggerpresent",
                r"check_debugger",
                r"anti_debug",
                r"detect_analysis",
            ],
            "log_cleanup": [
                r"clear_logs",
                r"remove_traces",
                r"clean_artifacts",
                r"erase_evidence",
            ],
            "trace_removal": [
                r"remove_footprint",
                r"clean_memory",
                r"unload_self",
                r"self_delete",
            ],
        }

        # Check for dangerous patterns
        for pattern in dangerous_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                self.logger.warning(f"Dangerous pattern found: {pattern}")
                return False

        # Calculate stealth score
        stealth_score = 0
        total_features = len(stealth_features)

        for feature, patterns in stealth_features.items():
            feature_found = any(
                re.search(pattern, code, re.IGNORECASE) for pattern in patterns
            )
            if feature_found:
                stealth_score += 1
                self.logger.info(f"Stealth feature found: {feature}")

        stealth_ratio = stealth_score / total_features
        self.logger.info(f"Stealth score: {stealth_ratio:.2f}")

        # Basic syntax check for Python code
        if code.startswith("#!/usr/bin/python") or code.startswith("import"):
            try:
                compile(code, "<string>", "exec")
            except SyntaxError:
                self.logger.error("Syntax error in generated code")
                return False

        # Require minimum stealth score
        MIN_STEALTH_SCORE = 0.6
        if stealth_ratio < MIN_STEALTH_SCORE:
            self.logger.warning(
                f"Insufficient stealth score: {stealth_ratio:.2f} < {MIN_STEALTH_SCORE}"
            )
            return False

        # All validation passed
        self.logger.info(
            "Code validation successful - all stealth and security requirements met"
        )
        return True

    def _analyze_code_complexity(self, lines: List[str]) -> float:
        """Analyze code complexity using various metrics"""
        complexity = 0.0

        # Control flow complexity
        control_flow_keywords = ["if", "else", "for", "while", "try", "except"]
        nesting_level = 0
        max_nesting = 0

        for line in lines:
            # Track nesting level
            indent_level = len(line) - len(line.lstrip())
            nesting_level = indent_level // 4  # Assuming 4-space indentation
            max_nesting = max(max_nesting, nesting_level)

            # Count control flow statements
            if any(keyword in line for keyword in control_flow_keywords):
                complexity += 0.1 * (nesting_level + 1)

        # Adjust for maximum nesting depth
        complexity += 0.2 * max_nesting

        return min(1.0, complexity)  # Normalize to [0,1]

    def _check_suspicious_patterns(self, code: str) -> bool:
        """Check for suspicious code patterns"""
        suspicious_patterns = [
            r"eval\s*\(",
            r"exec\s*\(",
            r"subprocess\.call",
            r"os\.system",
            r"__import__\s*\(",
            r"base64\.b64decode",
        ]

        return any(
            re.search(pattern, code, re.IGNORECASE) for pattern in suspicious_patterns
        )

    def _validate_security_requirements(self, code: str) -> bool:
        """Validate additional security requirements"""
        # Check for proper error handling
        if "try:" in code and "except:" not in code:
            self.logger.warning("Missing proper error handling")
            return False

        # Check for proper resource cleanup
        if ("open(" in code and "close()" not in code) or (
            "connect(" in code and "close()" not in code
        ):
            self.logger.warning("Missing proper resource cleanup")
            return False

        # Check for proper input validation
        if "input(" in code and not re.search(
            r"validate|check|sanitize", code, re.IGNORECASE
        ):
            self.logger.warning("Missing input validation")
            return False

        return True

    def _enhance_instruction(
        self, instruction: str, template: Optional[str] = None
    ) -> str:
        """Enhance generation instruction with context and learned patterns"""
        enhanced = f"Generate a safe proof-of-concept exploit for: {instruction}\n"
        if template:
            enhanced += f"\nTemplate reference:\n{template}\n"
        if self.learned_patterns:
            pattern_str = "\n".join([f"- {p}" for p in self.learned_patterns.keys()])
            enhanced += f"\nKnown patterns:\n{pattern_str}"
        return enhanced

    def generate_exploit(
        self, vulnerability_info: Dict, max_length: int = 512
    ) -> Optional[str]:
        """Generate exploit code based on vulnerability information using GPT and fallback models"""
        try:
            # Validate target
            if "target" not in vulnerability_info:
                self.logger.error("No target specified in vulnerability info")
                return None

            validation_result = self.target_validator.validate_target(
                vulnerability_info["target"],
                context={
                    "type": vulnerability_info.get("type"),
                    "severity": vulnerability_info.get("severity"),
                },
            )

            if not validation_result["allowed"]:
                self.logger.warning(
                    f"Target not in whitelist: {vulnerability_info['target']}"
                )
                return None

            # Log generation attempt
            self.logger.info(
                f"Generating exploit for target: {vulnerability_info['target']}"
            )

            # Find matching template
            template = self._find_matching_template(vulnerability_info)

            # Create enhanced instruction
            instruction = self._enhance_instruction(
                json.dumps(vulnerability_info), template
            )

            # Try GPT generation with learned patterns
            self.logger.info(
                "Attempting GPT-based exploit generation with learned patterns"
            )
            generated_code = self.gpt_generator.generate_exploit(vulnerability_info)

            # Update GPT generator's learning based on validation
            if generated_code:
                validation_result = self._validate_generated_code(generated_code)
                self.gpt_generator.update_learning(
                    vulnerability_info,
                    generated_code,
                    validation_result,
                    "Code validation " + ("passed" if validation_result else "failed"),
                )

                if validation_result:
                    self.logger.info(
                        "Successfully generated and validated exploit code"
                    )
                    return self.polymorphic_generator.generate_polymorphic_variant(
                        generated_code
                    )

            # Fall back to base model if GPT generation fails
            if not generated_code:
                self.logger.info("GPT generation failed, falling back to base model")
                print("Starting code generation with instruction:", instruction)

                # Generate code with base model
                try:
                    inputs = self.tokenizer.encode(instruction, return_tensors="pt")
                    print("Encoded input tokens:", len(inputs[0]))

                    self.logger.info("Starting model generation with parameters:")
                    generation_params = {
                        "max_length": max_length,
                        "num_beams": 3,
                        "no_repeat_ngram_size": 2,
                        "early_stopping": True,
                        "temperature": 0.7,
                        "top_k": 50,
                        "top_p": 0.95,
                        "do_sample": True,
                    }
                    self.logger.info(
                        f"Generation parameters: {json.dumps(generation_params)}"
                    )

                    outputs = self.model.generate(inputs, **generation_params)
                    self.logger.info(f"Generated output tokens: {len(outputs[0])}")

                    generated_code = self.tokenizer.decode(
                        outputs[0], skip_special_tokens=True
                    )
                    self.logger.info("Base model generation successful")
                except Exception as e:
                    self.logger.error(f"Error during base model generation: {str(e)}")
                    return None

            # Validate generated code
            print("Validating generated code...")
            validation_result = self._validate_generated_code(generated_code)
            print(f"Validation result: {validation_result}")

            if validation_result:
                self._update_metrics(True)
                self._learn_from_generation(vulnerability_info, generated_code)

                # Generate polymorphic variant
                try:
                    mutated_code = (
                        self.polymorphic_generator.generate_polymorphic_variant(
                            generated_code
                        )
                    )
                    self.logger.info("Generated polymorphic variant of exploit code")
                    return mutated_code
                except Exception as e:
                    self.logger.error(f"Error generating polymorphic variant: {str(e)}")
                    return generated_code  # Fallback to original code
            else:
                print("Code validation failed - unsafe patterns detected")
                self._update_metrics(False)
                return None

        except Exception as e:
            print(f"Error generating exploit: {e}")
            return None

    def _update_metrics(self, success: bool) -> None:
        """Update success metrics for continuous learning"""
        self.success_metrics["generated_count"] += 1
        if success:
            current_success = self.success_metrics["success_rate"] * (
                self.success_metrics["generated_count"] - 1
            )
            self.success_metrics["success_rate"] = (
                current_success + 1
            ) / self.success_metrics["generated_count"]
        else:
            current_success = self.success_metrics["success_rate"] * (
                self.success_metrics["generated_count"] - 1
            )
            self.success_metrics["success_rate"] = (
                current_success / self.success_metrics["generated_count"]
            )

    def _learn_from_generation(self, vuln_info: Dict, generated_code: str) -> None:
        """Learn patterns from successful generations with focus on stealth techniques"""

        def extract_behavior_patterns(code: str) -> Dict[str, List[str]]:
            """Extract behavioral patterns from code"""
            patterns = {
                "file_operations": re.findall(
                    r"(?:open|read|write|close)\s*\([^)]*\)", code
                ),
                "network_operations": re.findall(
                    r"(?:connect|send|recv|socket)\s*\([^)]*\)", code
                ),
                "process_operations": re.findall(
                    r"(?:fork|exec|spawn|system)\s*\([^)]*\)", code
                ),
                "memory_operations": re.findall(
                    r"(?:malloc|memcpy|mmap)\s*\([^)]*\)", code
                ),
                "system_calls": re.findall(r"(?:syscall|ioctl)\s*\([^)]*\)", code),
            }
            return {k: v for k, v in patterns.items() if v}

        # Extract potential patterns
        if "type" in vuln_info:
            vuln_type = vuln_info["type"]
            if vuln_type not in self.learned_patterns:
                self.learned_patterns[vuln_type] = {
                    "code_patterns": [],
                    "stealth_techniques": {
                        "process_hiding": [],
                        "memory_manipulation": [],
                        "anti_debugging": [],
                        "log_cleanup": [],
                        "trace_removal": [],
                    },
                }

            # Extract behavioral patterns
            behavior_patterns = extract_behavior_patterns(generated_code)
            if behavior_patterns:
                if "behavior_analysis" not in self.learned_patterns[vuln_type]:
                    self.learned_patterns[vuln_type]["behavior_analysis"] = {}
                self.learned_patterns[vuln_type]["behavior_analysis"].update(
                    behavior_patterns
                )

            # Extract code patterns
            lines = generated_code.split("\n")
            for line in lines:
                if len(line.strip()) > 10:  # Ignore very short lines
                    # Categorize stealth patterns
                    if any(
                        keyword in line.lower()
                        for keyword in ["process", "hide", "stealth"]
                    ):
                        self.learned_patterns[vuln_type]["stealth_techniques"][
                            "process_hiding"
                        ].append(line)
                    elif any(
                        keyword in line.lower()
                        for keyword in ["memory", "inject", "mmap"]
                    ):
                        self.learned_patterns[vuln_type]["stealth_techniques"][
                            "memory_manipulation"
                        ].append(line)
                    elif any(
                        keyword in line.lower()
                        for keyword in ["debug", "detect", "analysis"]
                    ):
                        self.learned_patterns[vuln_type]["stealth_techniques"][
                            "anti_debugging"
                        ].append(line)
                    elif any(
                        keyword in line.lower() for keyword in ["log", "clean", "clear"]
                    ):
                        self.learned_patterns[vuln_type]["stealth_techniques"][
                            "log_cleanup"
                        ].append(line)
                    elif any(
                        keyword in line.lower()
                        for keyword in ["trace", "remove", "unload"]
                    ):
                        self.learned_patterns[vuln_type]["stealth_techniques"][
                            "trace_removal"
                        ].append(line)
                    else:
                        self.learned_patterns[vuln_type]["code_patterns"].append(line)

            # Keep only unique patterns and limit storage
            for category in self.learned_patterns[vuln_type]["stealth_techniques"]:
                patterns = self.learned_patterns[vuln_type]["stealth_techniques"][
                    category
                ]
                patterns = list(set(patterns))
                if len(patterns) > 50:  # Limit per category
                    patterns = patterns[-50:]
                self.learned_patterns[vuln_type]["stealth_techniques"][
                    category
                ] = patterns

            # Limit general code patterns
            code_patterns = list(set(self.learned_patterns[vuln_type]["code_patterns"]))
            if len(code_patterns) > 100:
                code_patterns = code_patterns[-100:]
            self.learned_patterns[vuln_type]["code_patterns"] = code_patterns

            self.logger.info(
                f"Updated learned patterns for {vuln_type} with new stealth techniques"
            )

    def get_metrics(self) -> Dict:
        """Get current performance metrics"""
        return {
            "success_metrics": self.success_metrics,
            "learned_patterns_count": {
                k: len(v) for k, v in self.learned_patterns.items()
            },
            "total_patterns": sum(len(v) for v in self.learned_patterns.values()),
        }

    def enable_stealth_features(self, features: Dict[str, bool]) -> None:
        """Enable or disable stealth features

        Args:
            features: Dictionary of feature names and their enabled status
        """
        for feature, enabled in features.items():
            if feature in self.stealth_features:
                self.stealth_features[feature] = enabled
                self.logger.info(f"Stealth feature {feature} set to {enabled}")
            else:
                self.logger.warning(f"Unknown stealth feature: {feature}")


if __name__ == "__main__":
    # Example usage
    generator = ExploitGenerator()
    generator.load_templates("exploit_templates.csv")

    test_vuln = {
        "type": "sql_injection",
        "description": "SQL injection vulnerability in login form",
        "severity": "high",
    }

    exploit_code = generator.generate_exploit(test_vuln)
    if exploit_code:
        print("Generated exploit code:")
        print(exploit_code)
    else:
        print("Failed to generate safe exploit code")
